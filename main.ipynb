{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boardgame QA Debate Experiments\n",
    "\n",
    "This notebook runs experiments using the debate and judge modules on the BoardgameQA dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Set up logging configuration\n",
    "# Configure root logger\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\", level=logging.DEBUG\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "import re\n",
    "\n",
    "from debate.debate import DebateTwoPlayers\n",
    "from debate.judge import JudgeManager\n",
    "from debate.types import DebateScenario\n",
    "from debate.baseline import BaselineManager\n",
    "\n",
    "# Configure model list from most to least powerful\n",
    "MODELS = [\"gemini-1.5-pro\", \"gemini-1.5-flash\", \"gemini-1.5-flash-8b\"]\n",
    "\n",
    "# Dataset difficulty levels\n",
    "LEVELS = [\n",
    "    \"ZeroConflict\",\n",
    "    \"LowConflict\",\n",
    "    \"Main\",  # Medium\n",
    "    \"HighConflict\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_boardgame_qa(base_path: str = \"BoardgameQA\") -> dict:\n",
    "    \"\"\"Load BoardgameQA dataset from json files.\"\"\"\n",
    "    ds = {}\n",
    "    for level in LEVELS:\n",
    "        path = Path(base_path) / f\"BoardgameQA-{level}-depth2/test.json\"\n",
    "        with open(path) as f:\n",
    "            ds[level] = json.load(f)\n",
    "    return ds\n",
    "\n",
    "\n",
    "def sample_data(ds: dict, sample_size: float = 0.1) -> dict:\n",
    "    \"\"\"Sample data while maintaining label distribution.\"\"\"\n",
    "    sampled_data = {}\n",
    "    for level, data in ds.items():\n",
    "        label_to_examples = defaultdict(list)\n",
    "\n",
    "        for example in data:\n",
    "            label_to_examples[example[\"label\"]].append(example)\n",
    "\n",
    "        sample = []\n",
    "        for label, examples in label_to_examples.items():\n",
    "            sample_size_per_label = int(len(examples) * sample_size)\n",
    "            sample.extend(random.sample(examples, sample_size_per_label))\n",
    "\n",
    "        sampled_data[level] = sample\n",
    "    return sampled_data\n",
    "\n",
    "\n",
    "def convert_to_scenarios(examples: list) -> list[DebateScenario]:\n",
    "    \"\"\"Convert BoardgameQA examples to DebateScenario objects.\"\"\"\n",
    "\n",
    "    def split_question(example: str) -> Tuple[str, str]:\n",
    "        \"\"\"Split the example text into the game and the question.\"\"\"\n",
    "        example_text = example\n",
    "        question = re.split(r\"\\.\\s+\", example_text)[-1]\n",
    "        game = example_text.replace(question, \"\").strip()\n",
    "        return game, question\n",
    "\n",
    "    scenarios = []\n",
    "    for ex in examples:\n",
    "        situation, question = split_question(ex[\"example\"])\n",
    "        scenario = DebateScenario(\n",
    "            situation=situation,\n",
    "            question=question,\n",
    "            answer_options=[\"proved\", \"disproved\", \"unknown\"],\n",
    "            label=ex[\"label\"],\n",
    "        )\n",
    "        scenarios.append(scenario)\n",
    "    return scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and sample data\n",
    "ds = load_boardgame_qa()\n",
    "sampled_ds = sample_data(ds, sample_size=0.102)  # ~100 examples per level\n",
    "\n",
    "# Display sample sizes and label distributions\n",
    "for level, examples in sampled_ds.items():\n",
    "    print(f\"{level}: {len(examples)} examples\")\n",
    "    label_counts = Counter(ex[\"label\"] for ex in examples)\n",
    "    print(f\"  {dict(label_counts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiments\n",
    "\n",
    "We'll test different model configurations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    scenario: DebateScenario,\n",
    "    debater_models: list[str],\n",
    "    judge_models: list[str],\n",
    "    experiment_name: str,\n",
    "):\n",
    "    \"\"\"Run a complete experiment with debates, judgments and baseline.\"\"\"\n",
    "    Path(\"results\").mkdir(exist_ok=True)\n",
    "\n",
    "    # Run baseline first\n",
    "    baseline = BaselineManager(scenarios=[scenario], models=judge_models)\n",
    "    baseline.run()\n",
    "    baseline.save_results(f\"results/{experiment_name}_baseline.json\")\n",
    "\n",
    "    # Run debates\n",
    "    debate = DebateTwoPlayers(\n",
    "        scenario=scenario,\n",
    "        debater_models=debater_models,\n",
    "        word_limit=300,\n",
    "        max_debate_rounds=3,\n",
    "    )\n",
    "    debate.run()\n",
    "    debate.run(swap=True)\n",
    "    debate.run(all_wrong=True)\n",
    "    debate.save_results(f\"results/{experiment_name}_debates.json\")\n",
    "\n",
    "    # Run judgments\n",
    "    judge = JudgeManager(records=debate.get_records(), judge_models=judge_models)\n",
    "    judge.run()\n",
    "    judge.save_results(f\"results/{experiment_name}_judgments.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the experiment with a single scenario\n",
    "scenario = convert_to_scenarios(sampled_ds[\"Main\"])[0]\n",
    "debater_models = [MODELS[-1]] * 2\n",
    "judge_models = [MODELS[-1]]\n",
    "run_experiment(scenario, debater_models, judge_models, \"test2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sflkasjdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 1: Same capability debaters and judge\n",
    "for level in LEVELS:\n",
    "    scenarios = convert_to_scenarios(sampled_ds[level])\n",
    "    debater_models = [MODELS[0]] * 2  # Same model for both debaters\n",
    "    judge_model = [MODELS[0]]  # Same model for judge\n",
    "\n",
    "    exp_name = f\"{level}_same_capability\"\n",
    "    print(f\"Running {exp_name}...\")\n",
    "    run_experiment(scenarios, debater_models, judge_model, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 2: Stronger judge\n",
    "for level in LEVELS:\n",
    "    scenarios = convert_to_scenarios(sampled_ds[level])\n",
    "    debater_models = [MODELS[1]] * 2  # Weaker debaters\n",
    "    judge_model = [MODELS[0]]  # Stronger judge\n",
    "\n",
    "    exp_name = f\"{level}_stronger_judge\"\n",
    "    print(f\"Running {exp_name}...\")\n",
    "    run_experiment(scenarios, debater_models, judge_model, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration 3: Stronger debaters\n",
    "for level in LEVELS:\n",
    "    scenarios = convert_to_scenarios(sampled_ds[level])\n",
    "    debater_models = [MODELS[0]] * 2  # Stronger debaters\n",
    "    judge_model = [MODELS[1]]  # Weaker judge\n",
    "\n",
    "    exp_name = f\"{level}_stronger_debaters\"\n",
    "    print(f\"Running {exp_name}...\")\n",
    "    run_experiment(scenarios, debater_models, judge_model, exp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Load and analyze the experiment results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_results(pattern: str = \"results/*_judgments.json\") -> dict:\n",
    "    \"\"\"Load all experiment results matching pattern.\"\"\"\n",
    "    results = {}\n",
    "    for path in Path(\".\").glob(pattern):\n",
    "        with open(path) as f:\n",
    "            results[path.stem] = json.load(f)\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_results(results: dict):\n",
    "    \"\"\"Analyze judgment results across experiments.\"\"\"\n",
    "    analysis = defaultdict(dict)\n",
    "\n",
    "    for exp_name, exp_results in results.items():\n",
    "        for model, judgments in exp_results.items():\n",
    "            # Calculate accuracy, agreement rates, etc.\n",
    "            correct = sum(1 for j in judgments if j[\"judgment\"] == j[\"ground_truth\"])\n",
    "            total = len(judgments)\n",
    "            accuracy = correct / total if total > 0 else 0\n",
    "\n",
    "            analysis[exp_name][model] = {\n",
    "                \"accuracy\": accuracy,\n",
    "                \"total_judgments\": total,\n",
    "                \"correct_judgments\": correct,\n",
    "            }\n",
    "\n",
    "    return analysis\n",
    "\n",
    "\n",
    "# Load and analyze results\n",
    "results = load_results()\n",
    "analysis = analyze_results(results)\n",
    "\n",
    "# Display analysis\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame.from_dict(analysis, orient=\"index\")\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
